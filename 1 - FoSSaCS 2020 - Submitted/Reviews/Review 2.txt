----------------------- REVIEW 2 ---------------------
SUBMISSION: 93
TITLE: Decomposing Probabilistic Lambda-calculi
AUTHORS: Ugo Dal Lago, Giulio Guerrieri and Willem Heijltjes

----------- Overall evaluation -----------
SCORE: 1 (weak accept)
----- TEXT:
The paper considers the probabilistic lambda-calculus, an extension of the untyped lambda-calculus with a choice constructor $M+N$ reducing to $M$ or $N$ with equal probability 0.5. The operational semantics of this calculus is a Markov chain associating asymptotically a lambda-term with a subprobability distribution of normal forms. In fact, the calculus is usually endowed with a specific reduction strategy (cbn, cbv, etc), because the full reduction is not confluent: performing a choice $M+N$ before duplicating it produces a different distribution than duplicating $M+N$ and then choosing independently M or N in the various copies of $M+N$.

The paper proposes a new formalism which is claimed to recover confluence keeping the same expressiveness of  the probabilistic lambda-calculus. The idea is to decompose $M+N$ into two operators: a labelled superposition $+_a$ and a label binder $a.$. In my intuition, $M+_a N$ corresponds to a branching $if a then M else N$, while the binder $a.L$ is a sampling operator: $sample a from coin in L$, where $coin$ represents a fair coin tossing (so producing to $true$ or $false$ with equal probability $0.5$).

The standard choice operator $M+N$ is then represented by $a.(M+_aN)$. The crucial point is that the operational semantics will never  evaluate $a.(M+_aN)$ to M or N (i.e. you will never toss the coin $a.$), but it commutes the two new operators with the other syntactical constructs, so to create the redexes that would be generated by a probabilistic reduction. For example, $a.(\x.M+_aN) L$ reduces to $a.((\x.M)L+_a NL)$, producing the beta-redex $(\x.M)L$. The reduction relation is then a standard rewriting relation extending the beta-reduction, which is proven to enjoy good properties, such as confluence and simply typed strong normalization. The paper discusses also an encoding into this calculus of the standard call-by-value probabilistic lambda-calculus.

OVERALL EVALUATION

The idea of freezing the probabilistic choice reduction and approaching to the normal forms distribution via syntactic commutation rules is nice and original. The proofs of confluence and simply typed strong normalisation is based on standard techniques and is very sharp and enjoyable! I had no time to check the proofs in the details, but they seem likely.

However, I  should point two weaknesses that prevent me from a clear acceptance of this paper.

(1) The authors should compare more in detail their contribution with the existing literature (the shortcut "This is deeply different from what we do here" [p.3] is too coarse, in fact I disagree on this point). The idea of implementing probabilistic lambda-calculi by a deterministic rewriting enjoying a confluence property is already present in the literature, e.g. Faggian and Ronchi della Rocca's [12], but also (not cited) Alberti's PhD thesis "On operational properties of quantitative extensions of Î»-calculus" and (also not cited) the more recent Leventis' MSCS 2019 paper "A deterministic rewrite system for the probabilistic lambda-calculus". In particular, I am wondering about the differences between Levenits' approach and the one proposed in this paper. The fact that Leventis speaks about the "call-by-name" lambda-calculus should not confuse the reader, as in fact this corresponds to forbid the commutation of the binder operator $a.L$ in an argument position of an appl!
ication (as it is the case in your setting, p.5 line -3). Since Leventis' paper is very recent (online version appeared only this summer), I think it is not fair to discuss the acceptance of the submitted paper because of this lack of comparison, however I strongly recommend to add a more detailed discussion on this point in the final version.

(2) The second issue is more problematic from my point of view. The authors claim in the abstract that their calculus "interprets the call-by-name and call-by-value strategies", but, as far as I understand, the paper misses a clear account of these interpretations. Section 7 is devoted to a translation of cbv into their calculus, but I feel this one-page section sketched and much less accurate than the rest of the paper. In particular, Proposition 35 claims a kind of of simulation (M ->cbv N then [[M]]_cbv --> [[N]]_cbv), but  the authors do not discuss the converse (do we have some form of bisimulatiion?), neither the asymptotic behaviour of the semantics (how is related the big-step semantics of [[M]]_cbv with the normal forms of M?), even not with examples (e.g. $O(\x y+x)$, with O the cbv fix-point operator). Also, I do not see a similar discussion for the lazy cbn or the head-reduction. It might be possible that Section 6 describes a kind of simulation of the head-reduc!
tion, but this is too blurred and I would expect sharper statements (especially comparing with the clarity of the previous sections!)

To sum up, I would recommend to accept this paper only after a rewriting taking into account the previous points.


MINOR REMARKS:
- p.3 l.8: you can also mention : Clairambault and Paquet's "Fully Abstract Models of the Probabilistic lambda-calculus." CSL 2018 and Leventis and Pagani's "Strong Adequacy and Untyped Full-Abstraction for Probabilistic Coherence Spaces." FoSSaCS 2019, both giving the full abstract of the probabilistic untyped lambda-calculus

- p.6, prop.5: I think you should suppose that the two terms of a sum $P\oplus P'$ are different, otherwise you can apply the (i) reduction (the same for N).

- p.7, proof of Lemma 7: "multiset ordering": you can recall it. Also, the last rule of Figure 1 might change the labelling order (by extending it). Is this a problem for the proof of the lemma ? Even not, this point should be mentioned.

- p. 13 : Figure 5 -> Figure 4. Also, "modullo". Also, second line after Fig. 4: the reduction step should not yet have a $\theta$ apix.

- p.14, lemma 24: one SN occurrence should be erased.

- p.15, lemma 27: is it necessary to consider the context extension with $\Delta$?